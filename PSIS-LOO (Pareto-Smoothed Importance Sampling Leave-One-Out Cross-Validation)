import pymc as pm
import numpy as np
import pandas as pd
import arviz as az  # For PPC, LOO, and summaries

# Example data setup (replace with actual loading)
# df = pd.read_csv('pirls_data.csv')
# Assume columns: 'y' (score), 'country_idx', 'school_idx', 'class_idx'

# Coordinates
countries = np.unique(df['country_idx'])
schools = np.unique(df['school_idx'])
classes = np.unique(df['class_idx'])

coords = {
    "country": countries,
    "school": schools,
    "class": classes,
    "obs": np.arange(len(df))
}

with pm.Model(coords=coords) as pirls_four_level_non_centered_pymc:
    # Hyperpriors
    mu_global = pm.Normal("mu_global", mu=500, sigma=100)
    
    tau_country = pm.HalfCauchy("tau_country", beta=60)
    tau_school = pm.HalfCauchy("tau_school", beta=45)
    tau_class = pm.HalfCauchy("tau_class", beta=35)
    
    sigma_obs = pm.HalfNormal("sigma_obs", sigma=70)
    
    # Non-centered random effects
    z_country = pm.Normal("z_country", mu=0, sigma=1, dims="country")
    z_school = pm.Normal("z_school", mu=0, sigma=1, dims="school")
    z_class = pm.Normal("z_class", mu=0, sigma=1, dims="class")
    
    # Hierarchical means (broadcast to observations)
    mu_country = pm.Deterministic("mu_country", mu_global + z_country * tau_country, dims="country")
    
    mu_school = pm.Deterministic(
        "mu_school",
        mu_country[df['country_idx'].values] + z_school[df['school_idx'].values] * tau_school,
        dims="obs"
    )
    
    mu_class = pm.Deterministic(
        "mu_class",
        mu_school + z_class[df['class_idx'].values] * tau_class,
        dims="obs"
    )
    
    # Likelihood
    y_obs = pm.Normal(
        "y_obs",
        mu=mu_class,
        sigma=sigma_obs,
        observed=df['y'].values,
        dims="obs"
    )
    
    # Sampling
    trace = pm.sample(1000, tune=1000, target_accept=0.95, random_seed=42)

# Posterior Predictive Checks (Joy Explosion)
with pirls_four_level_non_centered_pymc:
    ppc = pm.sample_posterior_predictive(
        trace,
        var_names=["y_obs"],
        extend_inferencedata=True,
        random_seed=42
    )

# LOOCV Validation (PSIS-LOO Approximation – Efficient & Standard for Bayesian Models)
# ArviZ computes Pareto-Smoothed Importance Sampling Leave-One-Out CV (PSIS-LOO)
loo = az.loo(trace, pointwise=True)  # pointwise=True for detailed diagnostics

# Summary Output
print(loo)

# Key Interpretation
print(f"ELPD_LOO: {loo.loo:.2f} (± {loo.loo_se:.2f})")
print(f"Pareto k diagnostic summary:")
print(loo.pareto_k.describe())

# If any Pareto k > 0.7, consider refitting or exact LOO (computationally heavy)
# High k values indicate influential observations—inspect with az.plot_khat(loo)

# Visualization
az.plot_loo_pit(trace, legend=False)  # PIT for calibration check
az.plot_khat(loo)  # Pareto k values per observation

# Optional: Compare models if you have alternatives
# az.compare({"model1": trace1, "model2": trace2})
